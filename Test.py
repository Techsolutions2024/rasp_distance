import cv2
import time
import threading
import numpy as np
from ultralytics import YOLO
from collections import deque
import queue

class AccurateDetectionPipeline:
    def __init__(self, model_path="best_int8_openvino_model/", camera_id=0):
        # C·∫•u h√¨nh ƒë·ªÉ c√≥ bbox ch√≠nh x√°c
        self.CAMERA_RESOLUTION = (640, 480)  # Resolution g·ªëc t·ª´ camera
        self.DETECTION_RESOLUTION = (416, 320)  # Resolution cho YOLO
        self.SKIP_FRAMES = 0  # Kh√¥ng skip frame ƒë·ªÉ tracking li√™n t·ª•c
        self.BUFFER_SIZE = 2  # TƒÉng buffer ƒë·ªÉ kh√¥ng drop frame
        self.ALPHA = 0.8
        
        # Kh·ªüi t·∫°o camera calibration
        self._init_camera_calibration()
        
        # Kh·ªüi t·∫°o YOLO model
        self._init_yolo_model(model_path)
        
        # Queues v·ªõi buffer l·ªõn h∆°n
        self.frame_queue = queue.Queue(maxsize=3)
        self.detection_queue = queue.Queue(maxsize=2)
        
        # Control flags
        self.running = True
        
        # FPS tracking
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0
        self.detection_fps = 0
        
        # Pre-allocated buffers
        self.detection_buffer = np.zeros((*self.DETECTION_RESOLUTION[::-1], 3), dtype=np.uint8)
        
        self.camera_id = camera_id
        
        # ============ C·∫§U H√åNH DETECTION ============
        
        # OPTION 1: CH·ªà detect m·ªôt s·ªë class nh·∫•t ƒë·ªãnh
        # N·∫øu mu·ªën CH·ªà detect nh·ªØng class n√†y, uncomment d√≤ng d∆∞·ªõi:
        self.ALLOWED_CLASSES = {4,8,9, 3, 6,5, 7}  # ch·ªâ detect person, car, bus, truck
        #self.ALLOWED_CLASSES = None  # None = detect t·∫•t c·∫£ class
        
        # OPTION 2: C√°c class s·∫Ω t√≠nh kho·∫£ng c√°ch (c√≥ th·ªÉ kh√°c v·ªõi ALLOWED_CLASSES)
        self.CUSTOM_REAL_HEIGHTS = {
            #0: 1.7,   # person - chi·ªÅu cao ng∆∞·ªùi (m)
            #1: 1.6,   # bicycle
            #2: 4.0,   # 
            3: 3.2,   # bus
            4: 1.6,   # xe ƒë·∫°p
            5: 4.2,   # xe ƒë·∫ßu k√©o
            6: 3.2,   # xe du l·ªãch 
            7: 1.5,   # xe m√°y
            8: 2.3,   # √¥ t√¥
            9: 3.0,   # xe t·∫£i
            # Th√™m c√°c class kh√°c theo nhu c·∫ßu
            # 10: 2.0,  # fire hydrant
            # 11: 1.2,  # stop sign
            # ...
        }
        
        # ==========================================
        
        # T√≠nh to√°n scale factors ch√≠nh x√°c
        self.scale_x = self.CAMERA_RESOLUTION[0] / self.DETECTION_RESOLUTION[0]
        self.scale_y = self.CAMERA_RESOLUTION[1] / self.DETECTION_RESOLUTION[1]
        
    def _init_camera_calibration(self):
        """Kh·ªüi t·∫°o camera calibration v·ªõi t·ªça ƒë·ªô ch√≠nh x√°c"""
        try:
            data = np.load("camera_calib.npz")
            self.mtx = data["camera_matrix"]
            self.dist = data["dist_coeffs"]
            
            # T·∫°o undistort map cho camera resolution
            self.newcameramtx, self.roi = cv2.getOptimalNewCameraMatrix(
                self.mtx, self.dist, self.CAMERA_RESOLUTION, 0.3, self.CAMERA_RESOLUTION
            )
            
            # Map cho camera resolution
            self.camera_map1, self.camera_map2 = cv2.initUndistortRectifyMap(
                self.mtx, self.dist, None, self.newcameramtx, 
                self.CAMERA_RESOLUTION, cv2.CV_16SC2
            )
            
            # T√≠nh focal length
            f_x = self.newcameramtx[0, 0]
            f_y = self.newcameramtx[1, 1]
            self.focal_length = (f_x + f_y) / 2
            
            # T√≠nh ROI offset ƒë·ªÉ ƒëi·ªÅu ch·ªânh t·ªça ƒë·ªô
            self.roi_x_offset = self.roi[0]
            self.roi_y_offset = self.roi[1]
            
            self.use_calibration = True
            print(f"‚úì Camera calibration loaded - ROI: {self.roi}")
            
        except Exception as e:
            print(f"‚ö† Using default calibration: {e}")
            self.use_calibration = False
            self.focal_length = 350
            self.roi = (0, 0, self.CAMERA_RESOLUTION[0], self.CAMERA_RESOLUTION[1])
            self.roi_x_offset = 0
            self.roi_y_offset = 0
    
    def _init_yolo_model(self, model_path):
        """Kh·ªüi t·∫°o YOLO model"""
        print("üîÑ Loading YOLO model...")
        self.model = YOLO(model_path)
        
        # C·∫•u h√¨nh YOLO
        self.model.overrides.update({
            'verbose': False,
            'conf': 0.3,      # Gi·∫£m confidence ƒë·ªÉ detect nhi·ªÅu h∆°n
            'iou': 0.7,       
            'max_det': 20,    
            'device': 'cpu',
            'half': False,
            'agnostic_nms': False,
            'retina_masks': False,
        })
        
        print("‚úì YOLO model loaded")
    
    def camera_thread(self):
        """Thread ƒë·ªçc camera - kh√¥ng drop frame"""
        cap = cv2.VideoCapture(self.camera_id)
        
        # C·∫•u h√¨nh camera
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.CAMERA_RESOLUTION[0])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.CAMERA_RESOLUTION[1])
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        if not cap.isOpened():
            print("‚ùå Cannot open camera")
            self.running = False
            return
        
        print(f"‚úì Camera ready at {self.CAMERA_RESOLUTION}")
        
        while self.running:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # Undistort frame g·ªëc n·∫øu c·∫ßn
            if self.use_calibration:
                undistorted_frame = cv2.remap(frame, self.camera_map1, self.camera_map2, cv2.INTER_LINEAR)
                # Crop theo ROI
                x, y, w, h = self.roi
                processed_frame = undistorted_frame[y:y+h, x:x+w]
            else:
                processed_frame = frame
            
            # Put frame (blocking nh∆∞ng v·ªõi timeout)
            try:
                self.frame_queue.put(processed_frame, timeout=0.01)
            except queue.Full:
                # N·∫øu queue ƒë·∫ßy, l·∫•y frame c≈© ra v√† put frame m·ªõi
                try:
                    self.frame_queue.get_nowait()
                    self.frame_queue.put_nowait(processed_frame)
                except queue.Empty:
                    pass
        
        cap.release()
        print("üì∑ Camera stopped")
    
    def detection_thread(self):
        """Thread detection - x·ª≠ l√Ω m·ªçi frame"""
        while self.running:
            try:
                # L·∫•y frame
                frame = self.frame_queue.get(timeout=0.05)
                
                # Resize cho detection (gi·ªØ t·ª∑ l·ªá)
                detection_frame = cv2.resize(frame, self.DETECTION_RESOLUTION)
                
                # YOLO inference
                start_time = time.time()
                results = self.model(detection_frame, verbose=False)[0]
                inference_time = time.time() - start_time
                
                # T√≠nh detection FPS
                if inference_time > 0:
                    self.detection_fps = 1.0 / inference_time
                
                # X·ª≠ l√Ω detections v·ªõi t·ªça ƒë·ªô ch√≠nh x√°c
                detections = self._process_detections_accurate(results, frame.shape)
                
                # G·ª≠i k·∫øt qu·∫£
                detection_data = {
                    'frame': frame,  # Frame g·ªëc ƒë√£ undistort
                    'detections': detections,
                    'timestamp': time.time()
                }
                
                # Put detection (non-blocking)
                try:
                    # N·∫øu queue ƒë·∫ßy, drop detection c≈©
                    if self.detection_queue.full():
                        try:
                            self.detection_queue.get_nowait()
                        except queue.Empty:
                            pass
                    self.detection_queue.put_nowait(detection_data)
                except queue.Full:
                    pass
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Detection error: {e}")
                continue
        
        print("üéØ Detection stopped")
    
    def _process_detections_accurate(self, results, original_shape):
        """X·ª≠ l√Ω detections v·ªõi t·ªça ƒë·ªô ch√≠nh x√°c"""
        detections = []
        
        if results.boxes is None or len(results.boxes) == 0:
            return detections
        
        # T√≠nh scale factor ch√≠nh x√°c
        original_h, original_w = original_shape[:2]
        scale_x = original_w / self.DETECTION_RESOLUTION[0]
        scale_y = original_h / self.DETECTION_RESOLUTION[1]
        
        for box in results.boxes:
            cls_id = int(box.cls[0].item())
            conf = float(box.conf[0].item())
            
            if conf < 0.4:  # Threshold th·∫•p ƒë·ªÉ detect nhi·ªÅu
                continue
            
            # L·ªåC CLASS: Ch·ªâ gi·ªØ l·∫°i class ƒë∆∞·ª£c ph√©p (n·∫øu c√≥ set ALLOWED_CLASSES)
            if self.ALLOWED_CLASSES is not None and cls_id not in self.ALLOWED_CLASSES:
                continue  # Skip class n√†y
            
            # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô ch√≠nh x√°c
            x1, y1, x2, y2 = box.xyxy[0].tolist()
            
            # Scale v·ªÅ k√≠ch th∆∞·ªõc frame g·ªëc
            x1_scaled = int(x1 * scale_x)
            y1_scaled = int(y1 * scale_y)
            x2_scaled = int(x2 * scale_x)
            y2_scaled = int(y2 * scale_y)
            
            # ƒê·∫£m b·∫£o t·ªça ƒë·ªô trong bounds
            x1_scaled = max(0, min(x1_scaled, original_w - 1))
            y1_scaled = max(0, min(y1_scaled, original_h - 1))
            x2_scaled = max(x1_scaled + 1, min(x2_scaled, original_w))
            y2_scaled = max(y1_scaled + 1, min(y2_scaled, original_h))
            
            # T√≠nh kho·∫£ng c√°ch cho c√°c class ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
            distance = None
            if cls_id in self.CUSTOM_REAL_HEIGHTS:
                pixel_height = y2_scaled - y1_scaled
                if pixel_height > 10:
                    real_height = self.CUSTOM_REAL_HEIGHTS[cls_id]
                    # S·ª≠ d·ª•ng focal length ƒë√£ calibration
                    distance = self.ALPHA * (real_height * self.focal_length) / pixel_height
            
            detections.append({
                'bbox': (x1_scaled, y1_scaled, x2_scaled, y2_scaled),
                'class_id': cls_id,
                'confidence': conf,
                'label': cls_id,  # ‚úÖ THAY ƒê·ªîI: S·ª≠ d·ª•ng class_id thay v√¨ t√™n
                'distance': distance
            })
        
        return detections
    
    def display_thread(self):
        """Thread hi·ªÉn th·ªã"""
        while self.running:
            try:
                detection_data = self.detection_queue.get(timeout=0.1)
                
                frame = detection_data['frame'].copy()
                detections = detection_data['detections']
                
                # V·∫Ω detections
                self._draw_detections(frame, detections)
                
                # T√≠nh FPS
                self.fps_counter += 1
                current_time = time.time()
                
                if current_time - self.fps_start_time >= 1.0:
                    self.current_fps = self.fps_counter / (current_time - self.fps_start_time)
                    self.fps_counter = 0
                    self.fps_start_time = current_time
                
                # V·∫Ω th√¥ng tin
                self._draw_info(frame)
                
                # Hi·ªÉn th·ªã
                cv2.imshow("Accurate YOLO Detection", frame)
                
                # Check quit
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.running = False
                    break
                
            except queue.Empty:
                continue
        
        cv2.destroyAllWindows()
        print("üñ•Ô∏è Display stopped")
    
    def _draw_detections(self, frame, detections):
        """V·∫Ω detections v·ªõi th√¥ng tin ƒë·∫ßy ƒë·ªß - HI·ªÇN TH·ªä ID thay v√¨ t√™n"""
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            class_id = det['class_id']  # ‚úÖ L·∫•y class_id
            conf = det['confidence']
            distance = det['distance']
            
            # M√†u s·∫Øc theo class
            color = self._get_class_color(class_id)
            
            # V·∫Ω bbox
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # ‚úÖ THAY ƒê·ªîI: Hi·ªÉn th·ªã ID thay v√¨ t√™n
            if distance is not None:
                text = f"ID:{class_id} {conf:.2f} - {distance:.1f}m"
            else:
                text = f"ID:{class_id} {conf:.2f}"
            
            # Background cho text
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 1)[0]
            cv2.rectangle(frame, (x1, y1 - text_size[1] - 10), 
                         (x1 + text_size[0], y1), color, -1)
            
            # Text
            cv2.putText(frame, text, (x1, y1 - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    def _get_class_color(self, class_id):
        """L·∫•y m√†u cho t·ª´ng class v·ªõi mapping ch√≠nh x√°c"""
        # Mapping m√†u theo class_id c·ª• th·ªÉ
        color_map = {
            7: (255, 0, 255),    # ID:0 - magenta
            3: (0, 255, 0),      # ID:3 - green
            4: (255, 0, 0),      # ID:4 - blue  
            5: (0, 0, 255),      # ID:5 - red
            6: (0, 255, 0),    # ID:6 - cyan/yellow
            8: (0, 255, 0),    # ID:8 - yellow
            9: (128, 0, 128),    # ID:9 - purple
        }
        
        # Tr·∫£ v·ªÅ m√†u m·∫∑c ƒë·ªãnh n·∫øu class_id kh√¥ng c√≥ trong map
        return color_map.get(class_id, (128, 128, 128))  # Gray m·∫∑c ƒë·ªãnh
    
    def _draw_info(self, frame):
        """V·∫Ω th√¥ng tin FPS v√† status"""
        # FPS
        cv2.putText(frame, f"FPS: {self.detection_fps:.1f}", (10, 60),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
    
    def run(self):
        """Ch·∫°y pipeline"""
        print("üöÄ Starting accurate detection pipeline...")
        print(f"üìê Camera: {self.CAMERA_RESOLUTION}")
        print(f"üéØ Detection: {self.DETECTION_RESOLUTION}")
        print(f"üìè Scale factors: {self.scale_x:.2f}x, {self.scale_y:.2f}x")
        
        if self.ALLOWED_CLASSES is not None:
            print(f"üé™ Only detect class IDs: {sorted(list(self.ALLOWED_CLASSES))}")
        else:
            print(f"üé™ Detect ALL classes, distance for: {list(self.CUSTOM_REAL_HEIGHTS.keys())}")
        
        print(f"üìè Distance calculation for class IDs: {list(self.CUSTOM_REAL_HEIGHTS.keys())}")
        
        # Kh·ªüi ƒë·ªông threads
        threads = [
            threading.Thread(target=self.camera_thread, name="Camera"),
            threading.Thread(target=self.detection_thread, name="Detection"),
            threading.Thread(target=self.display_thread, name="Display")
        ]
        
        for thread in threads:
            thread.daemon = True
            thread.start()
        
        try:
            threads[2].join()  # Ch·ªù display thread
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è Stopping...")
            self.running = False
        
        # Cleanup
        for thread in threads[:-1]:
            thread.join(timeout=1.0)
        
        print("‚úÖ Pipeline stopped")

# S·ª≠ d·ª•ng
if __name__ == "__main__":
    pipeline = AccurateDetectionPipeline(
        model_path="best_int8_openvino_model/",
        camera_id=0
    )
    pipeline.run()
